---
title: "LASSO + Bayes Regression"
author: "Hannah Snell"
date: "12/10/2020"
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(glmnet)
library(monomvn)
library(miscTools)
library(mice)
library(rstan)
library(modelr)
library(bayesplot)
library(bayesrules)
library(lars)

set.seed(84735)

timeone_clean <- read_csv("timeone_clean.csv")
```

# Using the LASSO Algorithm and Bayesian Regression to best predict PPE use! 

_Using the `monomvm` package for Bayesian Regression and LASSO:_ 

## Template Code: 

```{r}
# Example code from Stack Overflow: 

burnin <- 500
iter <- 1000
initial.beta <- rep(-500, dim(x2)[2]) # assigning an extreme initial value for all betas
initial.lambda2 <- 10 # assigning an extreme initial value for lambda (penalty parameter)
initial.variance <- 500 # assigning an extreme initial value for variance parameter

# starting the Gibbs sampler here
lasso <- blasso(X = x2, # covariate matrix 
                y = y,  # response vector
                T = iter, # number of iterations
                beta = initial.beta, 
                lambda2 = initial.lambda2,  
                s2 = initial.variance)

# collecting draws for some of the parameters for visualization
coef.lasso <- as.data.frame(cbind(iter = seq(iter), 
                              beta1 = lasso$beta[, "b.1"], 
                              beta2 = lasso$beta[, "b.2"], 
                              variance = lasso$s2, 
                              lambda.square = lasso$lambda2))
```


```{r}
colMedians(coef.lasso[-seq(burnin), -1]) # new posterior mean estimations
```


The following results could be different if we specify our prior distributions instead of giving extreme values: 

```{r}
# Compare regular LASSO vs. BLASSO: number of coefficients shrunk to zero 

# Regular LASSO
fit.glmnet <-  glmnet(as.matrix(x2), y, 
                        lambda=cv.glmnet(as.matrix(x2), y)$lambda.1se)
coef.glmnet <- coef(fit.glmnet)
sum(coef.glmnet == 0)

# BLASSO
sum(colMedians(lasso$beta[-seq(burnin), ]) == 0)
```

If we get that all of our betas are shrunken to zero, try tuning hyperparameters (Park & Casella, 2008)

## Model Attempt (Hannah): 

We will have issues running LASSO with NA values present anywhere in our data. We want to keep as many rows as possible since our dataset is small, so we can use multiple imputation to fill in the missing value NAs with the mean of the particular variable. 

```{r}
# 1. Remove variables that we won't use: 
timeone_clean <- timeone_clean %>% 
  dplyr::select(-Occupation, -`ID_case#`, -`Date`)

# 2. Find the total NA's per variable: 
varsNA <- as.data.frame(sapply(timeone_clean, function(x) sum(is.na(x))))

# 3. Extra data cleaning before multiple imputation: 

# Isolate a small df of all the vars with NA's 
colnames(varsNA)[1] <- "NAs"
varsNA <- varsNA %>% 
  filter(NAs != 0)

# Clean all the vars with NA's (the ones that we won't imputate)
timeone_clean <- timeone_clean %>% 
  mutate(`SOC Category` = replace_na(`SOC Category`, "Unknown"), 
         State = replace_na(State, "Unknown"),  
         State = recode(State, "USA" = "Unknown"), 
         State = recode(State, "NONE" = "Unknown"),
         State = recode(State, "US" = "Unknown"), 
         State = recode(State, "GE" = "GA"),
         State = recode(State, "NEW  YORK" = "NY"), 
         State = recode(State, "34" = "Unknown"),
         Isolation_number_of_days = replace_na(Isolation_number_of_days, 0),  
         Religion = replace_na(Religion, "Unknown")) 

colnames(timeone_clean)[3] <- "SOC_Category"

timeone_clean <- timeone_clean %>% 
  mutate(State_dummy =  recode(State, 
                           "AL" = 1,
                           "AK" = 2,
                           "AZ" = 3,
                           "AR" = 4,
                           "CA" = 5,
                           "CO" = 6,
                           "CT" = 7,
                           "DE" = 8,
                           "DC" = 9,
                           "FL" = 10,
                           "GA" = 11,
                           "HI" = 12,
                           "ID" = 13,
                           "IL" = 14,
                           "IN" = 15, 
                           "IA" = 16,
                           "KS" = 17,
                           "KY" = 18,
                           "LA" = 19, 
                           "ME" = 20,
                           "MD" = 21,
                           "MA" = 22,
                           "MI" = 23,
                           "MN" = 24,
                           "MS" = 25,
                           "MO" = 26,
                           "MT" = 27,
                           "NE" = 28,
                           "NV" = 29,
                           "NH" = 30,
                           "NJ" = 31,
                           "NM" = 32,
                           "NY" = 33,
                           "NC" = 34,
                           "ND" = 35,
                           "OH" = 36,
                           "OK" = 37,
                           "OR" = 38,
                           "PA" = 39,
                           "RI" = 40,
                           "SC" = 41,
                           "SD" = 42,
                           "TN" = 43,
                           "TX" = 44,
                           "UT" = 45,
                           "VT" = 46,
                           "VA" = 47,
                           "WA" = 48,
                           "WV" = 49,
                           "WI" = 50,
                           "WY" = 51,
                           "Unknown" = 0),
         Religion_dummy = recode(Religion, 
                           "Protestant" = 1, 
                           "Roman Catholic" = 2,  
                           "Hindu" = 3, 
                           "Buddhist" = 4, 
                           "Mormon" = 5,  
                           "Jewish" = 6, 
                           "Agnostic" = 7, 
                           "Atheist" = 8, 
                           "Nothing in Particular" = 9, 
                           "Something Else" = 10, 
                           "Unknown" = 0
                           ),
         SOC_Category_dummy = recode(SOC_Category, 
                              "Management Occupations" = 1, 
                              "Business and Financial Operations Occupations" = 2, 
                              "Computer and Mathematical Occupations" = 3, 
                              "Architecture and Engineering Occupations" = 4, 
                              "Life, Physical, and Social Science Occupations" = 5, 
                              "Community and Social Service Occupations" = 6, 
                              "Legal Occupations" = 7, 
                              "Educational Instruction and Library Occupations" = 8, 
                              "Arts, Design, Entertainment, Sports, and Media Occupations" = 9,
                              "Healthcare Practitioners and Technical Occupations" = 10, 
                              "Healthcare Support Occupations" = 11, 
                              "Protective Service Occupations" = 12, 
                              "Food Preparation and Serving Related Occupations" = 13,  
                              "Building and Grounds Cleaning and Maintenance Occupations" = 14, 
                              "Personal Care and Service Occupations" = 15, 
                              "Sales and Related Occupations" = 16, 
                              "Office and Administrative Support Occupations" = 17, 
                              "Farming, Fishing, and Forestry Occupations" = 18, 
                              "Construction and Extraction Occupations" = 19, 
                              "Installation, Maintenance, and Repair Occupations" = 20, 
                              "Production Occupations" = 21, 
                              "Transportation and Material Moving Occupations" = 22, 
                              "Military Specific Occupations" = 23,  
                              "Unknown" = 0)
) %>% 
  dplyr::select(-SOC_Category, -State, -Religion)

```


```{r}
# 4. Imputate Age, Annual_income, Illnesses, Meds, Isolation_number_times_leave_per_day, Perceived_susceptibility_COVID: 
imputated_timeone <- mice(timeone_clean, m = 5, meth ='pmm', seed = 84735)
# summary(imputated_timeone)

p1 <- densityplot(imputated_timeone) # i think this is good? probably needs reviewing

complete_timeone <- complete(imputated_timeone, 1)
```


Before applying the model, I am assuming that we will need to do some sort of cross-validation, so I am going to split the dataset in half to make training and testing set for the algorithm:

```{r}
set.seed(84735)

samplesize <- 225

train_ind <- sample(nrow(complete_timeone), size = samplesize, replace = F, prob = NULL)

train <- complete_timeone[train_ind, ]
test <- complete_timeone[-train_ind, ]
```

Time for Bayesian LASSO:

```{r}
# This chunk takes a bit of time to run
set.seed(84735)

# convert categorical vars to factors before putting them in the model: 
# train <- train %>% 
#   mutate(SOC_Category = as.factor(SOC_Category), 
#          State = as.factor(State), 
#          Religion = as.factor(Religion),
#          Preventive_Action_Taken_Scale_PPEuse_Total = as.factor(Preventive_Action_Taken_Scale_PPEuse_Total)) #%>% 
 #dplyr::select(-SOC_Category, -State, -Religion) # <- removing categorical vars in case they are causing the issue

# Define complicated model for model matrix: 
model_formula <-
  "Preventive_Action_Taken_Scale_PPEuse_Total ~ Age + Gender + SOC_Category_dummy + Religion_dummy + State_dummy + Employment_status_Befor_COVID + Employment_status_After_COVID + Marital_status + Education + Race + Annual_income + Illnesses + Meds + Accomodation + Isolation_yesorno + Isolation_number_of_days + Isolation_number_times_leave_per_day + Perceived_susceptibility_COVID + FFMQdescribetotal + FFMQnonreacttotal + FFMQnonjudgetotal + FFMQobservetotal + FFMQawaretotal + FFMQtotal + FFMQnonjudgemean + FFMQnonreactmean + FFMQobservemean + FFMQawaremean + FFMQdescribemean + Patient_Health_Questionnaire_Total + Quality_of_Life_Total + General_Health_Questionnaire_Negative_Total + Perceived_Vulnerability_to_Disease_Total + Intolerance_of_Uncertainty_Total + Preventive_Action_Taken_Scale_Avoid_Travel_People_Total + Impact_of_Events_Scale_Total + Number_of_children" %>%
  as.formula()

# Model matrix assembly for train data: 
x_matrix_train <- train %>%
  model_matrix(model_formula, data = .) %>%
  dplyr::select(-`(Intercept)`) %>%
  as.matrix()

# parameter setup
burnin <- 5000
iter <- 10000
initial.beta <- rep(-500, dim(x_matrix_train)[2]) # assigning an extreme initial value for all betas
initial.lambda2 <- 10 # assigning an extreme initial value for lambda (penalty parameter)
initial.variance <- 500 # assigning an extreme initial value for variance parameter

# starting the Gibbs sampler here
suppressWarnings(lasso <- blasso(X = x_matrix_train, # covariate matrix
                y = train$Preventive_Action_Taken_Scale_PPEuse_Total,  # response vector
                T = iter, # number of iterations
                beta = initial.beta, 
                lambda2 = initial.lambda2,  
                s2 = initial.variance))

# collecting draws for some of the parameters for visualization
coef.lasso <- as.data.frame(cbind(iter = seq(iter), 
                              beta1 = lasso$beta[, "b.1"], 
                              beta2 = lasso$beta[, "b.2"], 
                              variance = lasso$s2, 
                              lambda.square = lasso$lambda2))
```

```{r}
colMedians(coef.lasso[-seq(burnin), -1]) # new posterior median estimations
```

Median $\lambda.square$ value is 0.4023588363 

### Diagnostics: 

(The following results could be different if we specify our prior distributions instead of giving extreme values:) 

Comparing vars dumped in regular LASSO
```{r}
# This needs some work here
# Compare regular LASSO vs. BLASSO: number of coefficients shrunk to zero 

# Regular LASSO
fit.glmnet <- glmnet(as.matrix(x_matrix_train), train$Preventive_Action_Taken_Scale_PPEuse_Total, 
                        lambda = cv.glmnet(as.matrix(x_matrix_train), train$Preventive_Action_Taken_Scale_PPEuse_Total)$lambda.1se)
coef.glmnet <- coef(fit.glmnet)
sum(coef.glmnet == 0)
# 33 vars dumped

# BLASSO
sum(colMedians(lasso$beta[-seq(burnin), ]) == 0)
# 30 vars dumped
```

```{r}
# made a results table based on nonzero slopes. even though there are technically 8, one of them 
coefficients <- tribble( 
  ~"var name", ~"beta", ~"median coef", ~"median tau2i", 
   "Age", "beta1", -0.0008406, 0.3313,
   "Religion_dummy", "beta4", -0.168963, 0.137869,
   "Employment_status_After_COVID", "beta7", -0.00788, 0.3448, 
   "Education", "beta9", 0.17121, 0.2283,
   "Annual_income", "beta11", 2.486e-06, 0.2679,
   "Intolerance_of_Uncertainty_Total", "beta35", 0.12965, 0.15457,
   "Preventive_Action_Taken_Scale_Avoid_Travel_People_Total", "beta36", 0.050522, 0.05764
  )
```

If we get that all of our betas are shrunken to zero, try tuning hyperparameters (Park & Casella, 2008).

Diagnostics: 

```{r}
plot(lasso, burnin = 5000)

summary(lasso, burnin = 5000)
```

Cross Validation: 

```{r}
cvlas <- cv.lars(x_matrix_train, train$Preventive_Action_Taken_Scale_PPEuse_Total, K = 10, type = "lasso")
frac <- cvlas$index[which.min(cvlas$cv)]
frac
las.coef <- predict.lars(lasso, type="coefficients", mode="fraction", s=frac)
las.coef
```


- do we use LASSO-based CV or Bayesian CV or a combo
- how can we visualize this

# References: 

[mice package](https://datascienceplus.com/imputing-missing-data-with-r-mice-package)
[park & casella, 2008](https://www-jstor-org.libproxy.smith.edu/stable/pdf/27640090.pdf?refreqid=excelsior%3A935888303bc6a810c32ec747294592bb)
[bayesian lasso guide, duke](http://www2.stat.duke.edu/~rcs46/lectures_2015/14-bayes1/14-bayes3.pdf)
[train & test set](https://stackoverflow.com/questions/17200114/how-to-split-data-into-training-testing-sets-using-sample-function)
[handling NAs in LASSO](https://stats.stackexchange.com/questions/152057/how-to-handle-na-values-in-shrinkage-lasso-method-using-glmnet/152179)
[counting NAs over all vars](https://sebastiansauer.github.io/sum-isna/)
[replace_na function](https://tidyr.tidyverse.org/reference/replace_na.html)
[blasso template code](https://stats.stackexchange.com/questions/268734/how-to-use-blasso-function-in-r-package-monomvn)

